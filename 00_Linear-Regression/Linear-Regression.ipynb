{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw1_regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9FfatPz6MU3"
      },
      "source": [
        "# **Linear Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsoaNwrZA0ui"
      },
      "source": [
        "**Goal**: leverage 18 features (including PM2.5) in the first 9 hours to predict PM2.5 in the 10th hour."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7RiAkkjCc6l"
      },
      "source": [
        "# **Load `train.csv`**\n",
        "- The `train.csv` file contains data from 12 months, 20 days of for each month, and 24 hours a day (hourly data with 18 features)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AfNX-hB3kN8"
      },
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive \n",
        "!gdown --id '1wNKAxQ29G15kgpBy_asjTcZRRgmsCZRm' --output data.zip\n",
        "!unzip data.zip\n",
        "# data = pd.read_csv('gdrive/My Drive/hw1-regression/train.csv', header = None, encoding = 'big5')\n",
        "data = pd.read_csv('./train.csv', encoding = 'big5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqUdj00pDTpo"
      },
      "source": [
        "# **Preprocessing** \n",
        "- Extract features.\n",
        "- Replace data in `RAINFALL` column with `0`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIGP7XUYD_Yb"
      },
      "source": [
        "data = data.iloc[:, 3:]\n",
        "data[data == 'NR'] = 0\n",
        "raw_data = data.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7PCrVwX6jBF"
      },
      "source": [
        "# **Extract Features (1)**\n",
        "![圖片說明](https://drive.google.com/uc?id=1LyaqD4ojX07oe5oDzPO99l9ts5NRyArH)\n",
        "![圖片說明](https://drive.google.com/uc?id=1ZroBarcnlsr85gibeqEF-MtY13xJTG47)\n",
        "\n",
        "- Regroup the original 4,320 data points (with 18-features) into 12 windows. Each window is one month containing data of 18 features and 480 hours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBnrGYXu9dZQ"
      },
      "source": [
        "month_data = {}\n",
        "for month in range(12):\n",
        "    sample = np.empty([18, 480])\n",
        "    for day in range(20):\n",
        "        sample[:, day * 24 : (day + 1) * 24] = raw_data[18 * (20 * month + day) : 18 * (20 * month + day + 1), :]\n",
        "    month_data[month] = sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhVmtFEQ9D6t"
      },
      "source": [
        "# **Extract Features (2)**\n",
        "![alt text](https://drive.google.com/uc?id=1wKoPuaRHoX682LMiBgIoOP4PDyNKsJLK)\n",
        "![alt text](https://drive.google.com/uc?id=1FRWWiXQ-Qh0i9tyx0LiugHYF_xDdkhLN)\n",
        "\n",
        "- There will be 480hrs-data (24 \\* 10) every month, and **a data** is formed every 9 hours. Also, there will be 471 data every month, so the total number of data is 471 \\* 12. Each data has 9 (hours) \\* 18 (features).\n",
        "- The corresponding target size is 471 \\* 12 (PM2.5 in the 10th hour)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcOrC4Fi-n3i"
      },
      "source": [
        "x = np.empty([12 * 471, 18 * 9], dtype = float)\n",
        "y = np.empty([12 * 471, 1], dtype = float)\n",
        "for month in range(12):\n",
        "    for day in range(20):\n",
        "        for hour in range(24):\n",
        "            if day == 19 and hour > 14:\n",
        "                continue\n",
        "            x[month * 471 + day * 24 + hour, :] = month_data[month][:,day * 24 + hour : day * 24 + hour + 9].reshape(1, -1) #vector dim:18*9 (9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9)\n",
        "            y[month * 471 + day * 24 + hour, 0] = month_data[month][9, day * 24 + hour + 9] #value\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wOii0TX8IwE"
      },
      "source": [
        "# **Normalize (1)**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceMqFoNI8ftQ"
      },
      "source": [
        "mean_x = np.mean(x, axis = 0) # 18 * 9 \n",
        "std_x = np.std(x, axis = 0) # 18 * 9 \n",
        "for i in range(len(x)): # 12 * 471\n",
        "    for j in range(len(x[0])): # 18 * 9 \n",
        "        if std_x[j] != 0:\n",
        "            x[i][j] = (x[i][j] - mean_x[j]) / std_x[j]\n",
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzvXP5Jya64j"
      },
      "source": [
        "# **Split Training Data Into `train_set` and `validation_set`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feF4XXOQb5SC"
      },
      "source": [
        "import math\n",
        "x_train_set = x[: math.floor(len(x) * 0.8), :]\n",
        "y_train_set = y[: math.floor(len(y) * 0.8), :]\n",
        "x_validation = x[math.floor(len(x) * 0.8): , :]\n",
        "y_validation = y[math.floor(len(y) * 0.8): , :]\n",
        "print(x_train_set)\n",
        "print(y_train_set)\n",
        "print(x_validation)\n",
        "print(y_validation)\n",
        "print(len(x_train_set))\n",
        "print(len(y_train_set))\n",
        "print(len(x_validation))\n",
        "print(len(y_validation))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-qAu0KR_ZRR"
      },
      "source": [
        "# **Training**\n",
        "![alt text](https://drive.google.com/uc?id=1xIXvqZ4EGgmxrp7c9r0LOVbcvd4d9H4N)\n",
        "![alt text](https://drive.google.com/uc?id=1S42g06ON5oJlV2f9RukxawjbE4NpsaB6)\n",
        "![alt text](https://drive.google.com/uc?id=1BbXu-oPB9EZBHDQ12YCkYqtyAIil3bGj)\n",
        "\n",
        "- Different from the above picture: the code below uses *Root Mean Square Error*.\n",
        "\n",
        "- Because of the existence of the constant term, dimension (`dim`) needs to be added one more column; the `eps` term is a minimal value added to avoid the denominator of adagrad being `0`.\n",
        "\n",
        "- Each dimension (`dim`) will correspond to its own gradient, weight (`w`), and learn through iteration (`iter_time`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCzDfxBFBFqp"
      },
      "source": [
        "dim = 18 * 9 + 1 # The dimension of all parameters, `+1` is to handle the bias\n",
        "w = np.zeros([dim, 1])\n",
        "# `np.ones` generates an array of 12*471 columns and 1 row and merges with the original features\n",
        "# That is, add a new vector to the first row to become an array with 12*471 columns and 18*9+1\n",
        "x = np.concatenate((np.ones([12 * 471, 1]), x), axis = 1).astype(float)\n",
        "\n",
        "learning_rate = 100\n",
        "iter_time = 1000\n",
        "adagrad = np.zeros([dim, 1]) # an array to update the learning rate for adagrad\n",
        "eps = 0.0000000001 # Avoid the denominator of iterative learning rate lr/sqrt(sum_of_pre_grads**2) becomes 0\n",
        "for t in range(iter_time):\n",
        "    loss = np.sqrt(np.sum(np.power(np.dot(x, w) - y, 2))/471/12) # rmse\n",
        "    if(t%100==0):\n",
        "        print(str(t) + \":\" + str(loss))\n",
        "    gradient = 2 * np.dot(x.transpose(), np.dot(x, w) - y) # dim*1\n",
        "    adagrad += gradient ** 2\n",
        "    w = w - learning_rate * gradient / np.sqrt(adagrad + eps)\n",
        "np.save('weight.npy', w)\n",
        "w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqNdWKsYBK28"
      },
      "source": [
        "# **Testing**\n",
        "![alt text](https://drive.google.com/uc?id=1165ETzZyE6HStqKvgR0gKrJwgFLK6-CW)\n",
        "\n",
        "- Load the test set and preprocess it in a manner similar to the training data, so that the test data forms 240 data with dimensions of 18 \\* 9 + 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AALygqJFCWOA"
      },
      "source": [
        "# testdata = pd.read_csv('gdrive/My Drive/hw1-regression/test.csv', header = None, encoding = 'big5')\n",
        "testdata = pd.read_csv('./test.csv', header = None, encoding = 'big5')\n",
        "test_data = testdata.iloc[:, 2:]\n",
        "test_data[test_data == 'NR'] = 0\n",
        "test_data = test_data.to_numpy()\n",
        "test_x = np.empty([240, 18*9], dtype = float)\n",
        "for i in range(240):\n",
        "    test_x[i, :] = test_data[18 * i: 18* (i + 1), :].reshape(1, -1)\n",
        "for i in range(len(test_x)):\n",
        "    for j in range(len(test_x[0])):\n",
        "        if std_x[j] != 0:\n",
        "            test_x[i][j] = (test_x[i][j] - mean_x[j]) / std_x[j]\n",
        "test_x = np.concatenate((np.ones([240, 1]), test_x), axis = 1).astype(float)\n",
        "test_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJQks9JEHR6W"
      },
      "source": [
        "# **Prediction**\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1165ETzZyE6HStqKvgR0gKrJwgFLK6-CW)\n",
        "\n",
        "- With weight and test set, the target can be predicted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNyB229jHsEQ"
      },
      "source": [
        "w = np.load('weight.npy')\n",
        "ans_y = np.dot(test_x, w)\n",
        "ans_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKMKW7RzHwuO"
      },
      "source": [
        "# **Save Prediction to CSV File**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dwfpqqy0H8en"
      },
      "source": [
        "import csv\n",
        "with open('submit.csv', mode='w', newline='') as submit_file:\n",
        "    csv_writer = csv.writer(submit_file)\n",
        "    header = ['id', 'value']\n",
        "    print(header)\n",
        "    csv_writer.writerow(header)\n",
        "    for i in range(240):\n",
        "        row = ['id_' + str(i), ans_y[i][0]]\n",
        "        csv_writer.writerow(row)\n",
        "        print(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y54yWq9cIPR4"
      },
      "source": [
        "Related reference:\n",
        "\n",
        "- [Adagrad](https://youtu.be/yKKNr-QKz2Q?list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&t=705)\n",
        "\n",
        "- [RMSprop](https://www.youtube.com/watch?v=5Yt-obwvMHI)\n",
        "\n",
        "- [Adam](https://www.youtube.com/watch?v=JXQT_vxqwIs)\n",
        "\n",
        "\n",
        "The `print` above is mainly to see the results, it doesn't hurt to remove it.\n",
        "\n",
        "Finally, you can surpass the baseline by adjusting the learning rate, iter_time (number of iterations), the number of features used (how many hours are taken, which feature fields to take), and even different models."
      ]
    }
  ]
}