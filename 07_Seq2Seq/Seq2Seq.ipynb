{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw8_seq2seq_stark.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.8-final"
    }
  },
  "cells": [
    {
      "source": [
        "# Sequence-to-Sequence Model (Seq2Seq)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVjWHnIv0LFS"
      },
      "source": [
        "# Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lDqKCy1xhrJ",
        "outputId": "ca3fc9a1-4725-4e2a-c936-7b40471ad4f0"
      },
      "source": [
        "!gdown --id '1r4px0i-NcrnXy1-tkBsIwvYwbWnxAhcg' --output data.tar.gz\n",
        "!tar -zxvf data.tar.gz\n",
        "!mkdir ckpt\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "'gdown' ���O�����Υ~���R�O�B�i���檺�{���Χ妸�ɡC\n",
            "tar: Error opening archive: Failed to open 'data.tar.gz'\n",
            "'ls' ���O�����Υ~���R�O�B�i���檺�{���Χ妸�ɡC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QdIJZCh0tdI"
      },
      "source": [
        "# Import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEdqUx_jxnAs"
      },
      "source": [
        "%%capture\n",
        "!pip3 install --user nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEWTGZ9c1N8D"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "\n",
        "import torch.utils.data as data\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw2nVpVzHvLV"
      },
      "source": [
        "## Define the Label Transform\n",
        "- Make padding to the label if it's in different length.\n",
        "- This operation is required for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gwTvmQCHu1j"
      },
      "source": [
        "class LabelTransform(object):\n",
        "  def __init__(self, size, pad):\n",
        "    self.size = size\n",
        "    self.pad = pad\n",
        "\n",
        "  def __call__(self, label):\n",
        "    label = np.pad(label, (0, (self.size - label.shape[0])), mode='constant', constant_values=self.pad)\n",
        "    return label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fmB_D9BOM0Z",
        "outputId": "a9ea4076-abc5-4519-c339-cd476b7b2e7a"
      },
      "source": [
        "class class1(object):\n",
        "  def __init__(self, size):\n",
        "    print(size)\n",
        "  def __call__(self, label):\n",
        "    print('111')\n",
        "    print(label)\n",
        "\n",
        "class1(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xctqzP5xQPBj",
        "outputId": "a3a164e0-3629-4deb-b4d8-cd242887fc02"
      },
      "source": [
        "a = class1(1)\n",
        "a(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUGgVG65Hskb"
      },
      "source": [
        "# Define the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjPOP7WwDQyN"
      },
      "source": [
        "class ENG2CNDataset(data.Dataset):\n",
        "  def __init__(self, root, max_output_len, set_name):\n",
        "    \"\"\"\n",
        "    root (string): the root directory of the dataset & dictionary.\n",
        "    max_output_len (int): the max output length of a sentence.\n",
        "    set_name (string): the dataset to be read. i.e. English or Chinese\n",
        "    \"\"\"\n",
        "    self.root = root\n",
        "    self.word2int_cn, self.int2word_cn = self.get_dictionary('cn')\n",
        "    self.word2int_en, self.int2word_en = self.get_dictionary('en')\n",
        "\n",
        "    # load the dataset\n",
        "    self.data = []\n",
        "    with open(os.path.join(self.root, f'{set_name}.txt'), 'r') as f:\n",
        "      for line in f:\n",
        "        self.data.append(line)\n",
        "    print(f'{set_name} dataset size: {len(self.data)}') # print the size of dataset\n",
        "\n",
        "    # get the dictionary size\n",
        "    self.cn_vocab_size = len(self.word2int_cn)\n",
        "    self.en_vocab_size = len(self.word2int_en)\n",
        "    # let the LabelTransform class instance behaves like a function\n",
        "    # for more info, plz refer to: https://www.geeksforgeeks.org/__call__-in-python/\n",
        "    self.transform = LabelTransform(max_output_len, self.word2int_en['<PAD>'])\n",
        "\n",
        "  def get_dictionary(self, language):\n",
        "    \"\"\"\n",
        "    Get the dictionary of the word2int and int2word for English and Chinese.\n",
        "    language (string): the language of the dictionary.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(self.root, f'word2int_{language}.json'), 'r') as f:\n",
        "      word2int = json.load(f)\n",
        "    with open(os.path.join(self.root, f'int2word_{language}.json'), 'r') as f:\n",
        "      int2word = json.load(f)\n",
        "    return word2int, int2word\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, Index):\n",
        "    # separate the Chinese and English sentence\n",
        "    # i.e separate the input and the label\n",
        "    # e.g. he is a teacher . \t他 是 老師 。 \n",
        "    # --> ['he is a teacher . ', '他 是 老師 。 ', ''] # len = 3\n",
        "    # --> ['he is a teacher . ', '他 是 老師 。 '] # len = 2\n",
        "    sentences = self.data[Index]\n",
        "    sentences = re.split('[\\t\\n]', sentences)\n",
        "    sentences = list(filter(None, sentences))\n",
        "    assert len(sentences) == 2\n",
        "\n",
        "    # prepare the special word\n",
        "    BOS = self.word2int_en['<BOS>']\n",
        "    EOS = self.word2int_en['<EOS>']\n",
        "    UNK = self.word2int_en['<UNK>']\n",
        "\n",
        "    # add <BOS> to the begining and <EOS> to the end\n",
        "    # for the unknown word, replace it with <UNK>\n",
        "    en, cn = [BOS], [EOS]\n",
        "    # separate the EACH sentence into subwords by English and Chinese separately\n",
        "    # e.g. ['he is a teacher . ', '他 是 老師 。 ']\n",
        "    # --> ['he', 'is', 'a', 'teacher', '.', ''] # len = 6\n",
        "    # --> ['he', 'is', 'a', 'teacher', '.'] # len = 5\n",
        "    # --> [1, 12, 11, 9, 215, 4, 2]\n",
        "    sentence = re.split(' ', sentences[0])\n",
        "    sentence = list(filter(None, sentence))\n",
        "    for word in sentence:\n",
        "      en.append(self.word2int_en.get(word, UNK))\n",
        "    en.append(EOS)\n",
        "\n",
        "    # Do the same for Chinese sentence\n",
        "    sentence = re.split(' ', sentences[1])\n",
        "    sentence = list(filter(None, sentence))\n",
        "    for word in sentence:\n",
        "      cn.append(self.word2int_cn.get(word, UNK))\n",
        "    cn.append(EOS)\n",
        "\n",
        "    en, cn = np.asarray(en), np.asarray(cn)\n",
        "\n",
        "    # make padding to the sentence to desired length\n",
        "    en, cn = self.transform(en), self.transform(cn)\n",
        "    en, cn = torch.LongTensor(en), torch.LongTensor(cn)\n",
        "\n",
        "    return en, cn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrUhUnm_Vv0h"
      },
      "source": [
        "# Model Structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m349MXsFV6Rk"
      },
      "source": [
        "## Encoder\n",
        "- It's a RNN model as an Encoder.\n",
        "- For each input, **Encoder** will output **a vector** and **a hidden state**, and use the hidden state for the next input.\n",
        "- In other words, the **Encoder**  will read the input sequence step by step, and give a single vector as output (final hidden state)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxyZU7nSTf_1"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  \"\"\"\n",
        "  input: the sentense that has been represented by integer sequence. i.e. [1, 12, 11, 9, 215, 4, 2]\n",
        "  output: \n",
        "    - the upper most layer output of RNN --> for attention\n",
        "    - the hidden state of each layer --> feed as the input for decoder\n",
        "  \"\"\"\n",
        "  def __init__(self, en_vocab_size, emb_dim, enc_hid_dim, dec_hid_dim, n_layers, dropout):\n",
        "    \"\"\"\n",
        "    en_vocab_size: the size of the dictionary. i.e. the # of the subwords\n",
        "    emb_dim: the dimension of embedding that is transformed from each word\n",
        "    enc_hid_dim: the dimension of encoder hidden layer and hidden state\n",
        "    dec_hid_dim: the dimension of decoder hidden state\n",
        "    n_layers: the # of layers in RNN\n",
        "    dropout: the percentage of dropout rate\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(en_vocab_size, emb_dim)\n",
        "    self.enc_hid_dim = enc_hid_dim # comment?\n",
        "    self.dec_hid_dim = dec_hid_dim # comment?\n",
        "    self.n_layers = n_layers # comment?\n",
        "    self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True, bidirectional=True)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.fc = nn.Linear(enc_hid_dim*2, dec_hid_dim)\n",
        "\n",
        "  def forward(self, input):\n",
        "    # input shape: [batch_size, sentence_length, vocab size]\n",
        "    embedding = self.embedding(input) # learnable\n",
        "    outputs, hidden = self.rnn(self.dropout(embedding))\n",
        "    # outputs shape: [batch_size, sentence_length, hid_dim * directions]\n",
        "    # hidden shape: [num_layers * directions, batch_size, hid_dim]\n",
        "    # outputs is the most upper layer output\n",
        "    batch_size = outputs.shape[0]\n",
        "    s = hidden.view(self.n_layers, 2, batch_size, -1) # s is the hidden states from Encoder\n",
        "    # s shape: [layers, directions, batch_size, enc_hid_dim]\n",
        "    s = torch.cat((s[-1, -2, :, :], s[-1, -1, :, :]), dim=1) # concatenate the 2 hidden states from the bidirectional RNN from Encoder\n",
        "    s = torch.tanh(self.fc(s)) # shape: [batch size, dec_hid_dim]\n",
        "\n",
        "    return outputs, s, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Attention"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        \"\"\"\n",
        "        enc_hid_dim: the dimension of encoder hidden layer and hidden state\n",
        "        dec_hid_dim: the dimension of decoder hidden state\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(enc_hid_dim * 2 + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, s, enc_output):\n",
        "        \"\"\"\n",
        "        hidden: the hidden layer of encoder # hidden shape: [batch size, num_layers * directions, enc_hid_dim]\n",
        "        enc_output: the output of encoder\n",
        "        max_output_len: the sentence length\n",
        "        \"\"\"\n",
        "        # enc_output shape: [batch_size, sentence_length, hid_dim * directions] = [batch_size, sentence_length, hid_dim * 2]\n",
        "        batch_size = enc_output.shape[0]\n",
        "        sentence_len = enc_output.shape[1]\n",
        "        # s shape: [batch_size, dec_hid_dim]\n",
        "        # repeat decoder hidden state for 'sentence_length' times\n",
        "        # s_new shape: [batch_size, sentence_length, dec_hid_dim]\n",
        "        s_new = s.unsqueeze(1).repeat(1, sentence_len, 1)\n",
        "        # energy shape: [batch_size, sentence_length, dec_hid_dim]\n",
        "        energy = self.attn(torch.cat((s_new, enc_output), dim = 2)) # concatenate along the dimension of 'dec_hid_dim'\n",
        "\n",
        "        # attention shape: [batch_size, sentence_length]\n",
        "        attention = self.v(energy).squeeze()\n",
        "\n",
        "        return F.softmax(attention, dim = 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7oX_QC9X5Hd"
      },
      "source": [
        "tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(tensor)\n",
        "print(tensor.shape)\n",
        "print()\n",
        "print(tensor.unsqueeze(1))\n",
        "print(tensor.unsqueeze(1).shape)\n",
        "print()\n",
        "print(tensor.unsqueeze(1).repeat(1, 2, 1))\n",
        "print(tensor.unsqueeze(1).repeat(1, 2, 1).shape)\n",
        "print()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3],\n        [4, 5, 6]])\ntorch.Size([2, 3])\n\ntensor([[[1, 2, 3]],\n\n        [[4, 5, 6]]])\ntorch.Size([2, 1, 3])\n\ntensor([[[1, 2, 3],\n         [1, 2, 3]],\n\n        [[4, 5, 6],\n         [4, 5, 6]]])\ntorch.Size([2, 2, 3])\n\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# Decoder\n",
        "- Introduction\n",
        "    - Decoder is built by RNN model.\n",
        "    - For the Decoder in the most simple Seq2Seq model, we only use the **LAST** hidden output from Encoder. This hidden output is also called \"Content Vector\".\n",
        "    - The Content Vector can be seen as the encoded vector based on what it has read from the previos context.\n",
        "    - The Content Vector is used as the initial condition for Decoder.\n",
        "    - For the Encoder outputs, they are used in the Attention mechanism.\n",
        "\n",
        "- Inputs\n",
        "    - The previos encoded word embedding represented as integer.\n",
        "- Outputs\n",
        "    - hidden: the updated status of the hidden state based on previos inputs and hidden state\n",
        "    - output: the percentage of the current word output\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, cn_vocab_size, emb_dim, enc_hid_dim, dec_hid_dim, n_layers, dropout, isatt):\n",
        "        \"\"\"\n",
        "        en_vocab_size: the size of the dictionary. i.e. the # of the subwords\n",
        "        emb_dim: the dimension of embedding that is transformed from each word\n",
        "        enc_hid_dim: the dimension of encoder hidden layer and hidden state\n",
        "        dec_hid_dim: the dimension of decoder hidden state\n",
        "        n_layers: the # of layers in RNN\n",
        "        dropout: the percentage of dropout rate\n",
        "        isatt: determine if using Attention or notq\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.cn_vocab_size = cn_vocab_size\n",
        "        self.hid_dim = dec_hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.isatt =  isatt\n",
        "        # the input dimension to the Decoder will be changed if using Attention\n",
        "        self.input_dim = (enc_hid_dim * 2 + emb_dim) if isatt else emb_dim\n",
        "        self.attention = Attention(enc_hid_dim, dec_hid_dim)\n",
        "        self.embedding = Embedding(cn_vocab_size, emb_dim)\n",
        "        self.rnn = nn.GRU(self.input_dim, self.dec_hid_dim, self.n_layers, dropout=dropout, batch_first=True)\n",
        "        self.embedding2vocab1 = nn.Linear(self.hid_dim, self.hid_dim*2)\n",
        "        self.embedding2vocab2 = nn.Linear(self.hid_dim*2, self.hid_dim*4)\n",
        "        self.embedding2vocab3 = nn.Linear(self.hid_dim*4, self.cn_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, dec_input, s, enc_output):\n",
        "        \"\"\"\n",
        "        dec_input: the input to the decoder. [batch_size, vocab_size]\n",
        "        s: the output vector from Attention. [batch_size, dec_hid_dim]\n",
        "        enc_output: the output from encoder. [batch_size, sentence_length, enc_hid_dim * 2]\n",
        "        \"\"\"\n",
        "        # the direction in decoder will only be single direction --> directions = 1\n",
        "        dec_input = dec_input.unsqueeze(1)\n",
        "        # embedded shape: [batch_size, 1, emb_dim]\n",
        "        embedded = self.dropout(self.embedding(dec_input))\n",
        "        if self.isatt:\n",
        "            alpha = self.attention(s, enc_output) # [batch_size, sentence_length]\n",
        "            alpha = alpha.unsqueeze(1) # [batch_size, 1, sentence_length]\n",
        "            c = torch.bmm((alpha, enc_output), dim=2) # weighted-sum\n",
        "            rnn_input = torch.cat((c, embedded), dim=2)\n",
        "        else:\n",
        "            rnn_input = embedded\n",
        "        \n",
        "        dec_output, dec_hidden = self.rnn(rnn_input, dec_hidden)\n",
        "        # dec_output shape: [batch_size, 1, hid_dim]\n",
        "        # dec_hidden shape: [num_layers, batch_size, hid_dim]\n",
        "\n",
        "        # transform the word vector to the percentage of occurring\n",
        "        # dec_output shape: [batch, hid_dim]\n",
        "        dec_output = dec_output.squeeze()\n",
        "        dec_output = self.embedding2vocab1(dec_output)\n",
        "        dec_output = self.embedding2vocab2(dec_output)\n",
        "        prediction = self.embedding2vocab3(dec_output) # [batch_size, vocab_size]\n",
        "        return prediction, dec_hidden\n"
      ]
    },
    {
      "source": [
        "# Seq2Seq\n",
        "- Constructed by Encoder and Decoder."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "    encoder: the encoder model\n",
        "    decoder: the decoder model\n",
        "    device: CPU or GPU\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        # encoder and decoder must have the equal number of layers\n",
        "        assert encoder.n_layers == decoder.n_layers\n",
        "\n",
        "    def forward(self, input, target, teacher_forcing_ratio):\n",
        "        \"\"\"\n",
        "        input: the input sentence [batch_size, input len, vocab_size]\n",
        "        target: the label (correct translated language) of the sentence [batch_size, target len, vocab_size]\n",
        "        teacher_forcing_ratio: the percentage of using label for training\n",
        "        \"\"\"\n",
        "        batch_size = target.shape[0]\n",
        "        target_len = target.shape[1]\n",
        "        vocab_size = self.decoder.cn_vocab_size\n",
        "\n",
        "        # initiate a torch tensor for storing the answer\n",
        "        outputs = torch.zeros(batch_size, target_len, vocab_size).to(self.device)\n",
        "        # put the input to the encoder\n",
        "        encoder_outputs, s, hidden = self.encoder(input)\n",
        "        # get the <BOS> token\n",
        "        input = target[:, 0] # the input for the decoder at the beginning\n",
        "        preds = []\n",
        "        \n",
        "        # iteratively input the enc_output to the decoder\n",
        "        for t in range(1, target_len):\n",
        "            output, hidden = self.decoder(input, s, encoder_outputs)\n",
        "            outputs[:, t] = output\n",
        "\n",
        "            # initialize the teacherforcing ratio\n",
        "            teacher_force = random.random() <= teacher_forcing_ratio\n",
        "\n",
        "            # get the word with the maximum percentage\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            # train with label if teacher forcing else using the predicted word\n",
        "            # update the input for next timestamp\n",
        "            input = targer[:, t] if teacher_force and t < target_len else top1\n",
        "            preds = preds.append(top1.unsqueeze(1))\n",
        "        preds = torch.cat(preds, 1)\n",
        "        return outputs, preds\n",
        "\n",
        "\n",
        "    def inference(self, input, target):\n",
        "        \"\"\"\n",
        "        When testing, not using the label as input for decoder.\n",
        "        input: the input sentence [batch_size, input len, vocab_size]\n",
        "        target: the label (correct translated language) of the sentence [batch_size, target len, vocab_size]\n",
        "        \"\"\"\n",
        "        batch_size = target.shape[0]\n",
        "        target_len = target.shape[1]\n",
        "        vocab_size = self.decoder.cn_vocab_size\n",
        "\n",
        "        # initiate a torch tensor for storing the answer\n",
        "        outputs = torch.zeros(batch_size, target_len, vocab_size).to(self.device)\n",
        "        # put the input to the encoder\n",
        "        encoder_outputs, s, hidden = self.encoder(input)\n",
        "        # get the <BOS> token\n",
        "        input = target[:, 0] # the input for the decoder at the beginning\n",
        "        preds = []\n",
        "        \n",
        "        # iteratively input the enc_output to the decoder\n",
        "        for t in range(1, target_len):\n",
        "            output, hidden = self.decoder(input, s, encoder_outputs)\n",
        "            outputs[:, t] = output\n",
        "\n",
        "            # initialize the teacherforcing ratio\n",
        "            teacher_force = random.random() <= teacher_forcing_ratio\n",
        "\n",
        "            # get the word with the maximum percentage\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            # train with label if teacher forcing else using the predicted word\n",
        "            # update the input for next timestamp\n",
        "            input = top1\n",
        "            preds = preds.append(top1.unsqueeze(1))\n",
        "        preds = torch.cat(preds, 1)\n",
        "        return outputs, preds"
      ]
    },
    {
      "source": [
        "# utils\n",
        "- Basic operations\n",
        "    - Save the model\n",
        "    - Load the model\n",
        "    - Construct the model\n",
        "    - Transform the digit sequence to word sentence\n",
        "    - Calculate the BELU score\n",
        "    - Iterate through the DataLoader"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## Save the model"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_model(model, store_model_path, step):\n",
        "    torch.save(model.state_dict(), f'{store_model_path}/model_{step}.ckpt')\n",
        "    return"
      ]
    },
    {
      "source": [
        "## Load the model"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model(model, load_model_path):\n",
        "    print(f'Load the model from {load_model_path}')\n",
        "    model.load_state_dict(torch.load(f'{load_model_path}.ckpt'))\n",
        "    return model"
      ]
    },
    {
      "source": [
        "## Construct the model"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model(config, en_vocab_size, cn_vocab_size):\n",
        "    # build the model\n",
        "    encoder = Encoder(en_vocab_size, config.emb_dim, config.enc_hid_dim, config.dec_hid_dim, config.n_layers, config.dropout)\n",
        "    decoder = Decoder(cn_vocab_size, config.emb_dim, config.enc_hid_dim, config.dec_hid_dim, config.n_layers, config.dropout, config.attention)\n",
        "    model = Seq2Seq(encoder, decoder, device)\n",
        "\n",
        "    # define the optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "    print(optimizer)\n",
        "    if config.load_model:\n",
        "        model = load_model(model, config.load_model_path)\n",
        "    model = model.to(device)\n",
        "\n",
        "    return model, optimizer"
      ]
    },
    {
      "source": [
        "## Transform the digit sequence to word sentence"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokens2sentence(outpus, int2word):\n",
        "    sentences = []\n",
        "    for tokens in outputs:\n",
        "        sentence = []\n",
        "        for token in tokens:\n",
        "            word = int2word[str(int(token))]\n",
        "            if word == '<EOS>':\n",
        "                break\n",
        "            sentence.append(word)\n",
        "        sentences.append(sentence)\n",
        "    return sentences"
      ]
    },
    {
      "source": [
        "## Calculate the BELU score"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def computebleu(sentences, targets):\n",
        "    smooth = SmoothingFunction() # smooth the log function\n",
        "    score = 0\n",
        "    assert (len(sentences) == len(targets))\n",
        "\n",
        "    def cut_token(sentence):\n",
        "        \"\"\"\n",
        "        Cut the vocabulary into word.\n",
        "        \"\"\"\n",
        "        tmp = []\n",
        "        for token in sentence:\n",
        "            if token == 'UNK' or token.isdigit() or len(bytes(token[0], encoding='utf-8')) == 1: # no need to cut if the token is digit or alphabet\n",
        "                tmp.append(token)\n",
        "            else:\n",
        "                tmp += [word for word in token]\n",
        "        return tmp\n",
        "    \n",
        "    for sentence, target in zip(sentences, targets):\n",
        "        sentence = cut_token(sentence)\n",
        "        target = cut_token(target)\n",
        "        score += sentence_bleu([target], sentence, weights=[0.25, 0.25, 0.25, 0.25], smoothing_function=smooth.method1)\n",
        "\n",
        "    return score"
      ]
    },
    {
      "source": [
        "## Iterate through the DataLoader"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inifite_iter(data_loader):\n",
        "    it = iter(data_loader)\n",
        "    while True:\n",
        "        try:\n",
        "            ret = next(it)\n",
        "            yield ret\n",
        "        except StopIteration:\n",
        "            it = iter(data_loader)"
      ]
    },
    {
      "source": [
        "# Training Process"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "## Training"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, optimizer, train_iter, loss_function, total_steps, summary_steps, teacher_forcing_ratio):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    losses = []\n",
        "    loss_sum = 0.0\n",
        "\n",
        "    for step in range(summary_steps):\n",
        "        sources, targets = next(train_iter)\n",
        "        sources, targets = sources.to(device), targets.to(device)\n",
        "        outputs, preds = model(sources, targets, teacher_forcing_ratio)\n",
        "        # outputs shape: [batch_size, sentence_length, vocab_size]\n",
        "        # preds shape: [batch_size, sentence_length]\n",
        "        \n",
        "        # ignore the 1st token of targets since it's <BOS>\n",
        "        outputs = outputs[:, 1:].reshape(-1, outputs.size(2))\n",
        "        # outputs shape: [batch_size * sentence_length, vocab_size]\n",
        "        targets = targets[:, 1:].reshape(-1, targets)\n",
        "        # targets shape: [batch_size * sentence_length]\n",
        "        loss = loss_function(outputs, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # avoid gradient explosion\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_sum += loss.item()\n",
        "\n",
        "        if (step + 1) % 5 == 0:\n",
        "            loss_sum = loss_sum / 5\n",
        "                  print ('\\r', 'train [{}] loss: {:.3f}, Perplexity: {:.3f}      '.format(total_steps + step + 1, loss_sum, np.exp(loss_sum)), end=' ')\n",
        "            losses.append(loss_sum)\n",
        "            loss_sum = 0.0\n",
        "    return model, optimizer, losses"
      ]
    },
    {
      "source": [
        "## Testing"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test(model, dataloader, loss_function):\n",
        "    model.eval() # set to evaluation mode\n",
        "    loss_sum, bleu_score = 0.0, 0.0\n",
        "    n = 0\n",
        "    result = []\n",
        "\n",
        "    for sources, targets in dataloader:\n",
        "        sources, targets = sources.to(device), targets.to(device)\n",
        "        batch_size = sources.size(0)\n",
        "        outputs, preds = model.inference(sources, targets)\n",
        "\n",
        "        # trim out the 1st character since it's <BOS>\n",
        "        outputs = outputs[:, 1:, ].reshape(-1, outputs.size(2))\n",
        "        # outputs shape: [batch_size * sentence_length, vocab_size]\n",
        "        targets = targets[:, 1:].reshape(-1)\n",
        "        # targets shape: [batch_size * sentence_length]\n",
        "\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss_sum += loss.item()\n",
        "\n",
        "        # transform the prediction into word\n",
        "        targets = targets.view(batch_size, -1)\n",
        "        # targets shape: [batch_size, vocab size]\n",
        "\n",
        "        # translate the prediction\n",
        "        preds = tokens2sentence(preds, dataloader.dataset.int2word_cn)\n",
        "        # preds shape: [batch_size, sentence_length]\n",
        "\n",
        "        # the sentence to be translated\n",
        "        sources = tokens2sentence(sources, dataloader.dataset.int2word_en)\n",
        "\n",
        "        # the label\n",
        "        targets = tokens2sentence(targets, dataloader.dataset.int2word_cn)\n",
        "\n",
        "        for source, pred, target in zip(targets, preds, targets):\n",
        "            result.append((source, pred, target))\n",
        "        \n",
        "        # compute the bleu score\n",
        "        bleu_score += computebleu(preds, targets)\n",
        "        n += batch_size\n",
        "    \n",
        "    return loss_sum / len(dataloader), batch_size / n, result"
      ]
    },
    {
      "source": [
        "## Training pipeline"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_process(config):\n",
        "    # prepare the training data\n",
        "    train_dataset = ENG2CNDataset(config.data_path, config.max_output_len, 'training')\n",
        "    train_loader = data.DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "    train_iter = inifite_iter(train_loader)\n",
        "\n",
        "    # prepare the validation data\n",
        "    val_dataset = ENG2CNDataset(config.data_path, config.max_output_len, 'validation')\n",
        "    val_loader = data.DataLoader(val_dataset, batch_size=2) # can not set the batch_size = 1 since PyTorch will downgrade the dimension automatically\n",
        "\n",
        "    # build the model\n",
        "    model, optimizer = build_model(config, train_dataset.en_vocab_size, train_dataset.cn_vocab_size)\n",
        "    loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    train_losses, val_losses, bleu_scores = [], [], []\n",
        "    total_steps = 0\n",
        "\n",
        "    while (total_steps < config.num_steps):\n",
        "        # training\n",
        "        model, optimier, loss = train(model, optimizer, train_iter, loss_function, total_steps, config.summary_steps, teacher_forcing_ratio=teacher_forcing_ratio)\n",
        "        train_losses += loss\n",
        "        # validation\n",
        "        val_loss, bleu_score, result = test(model, val_loader, loss_function)\n",
        "        val_losses.append(val_loss)\n",
        "        bleu_scores.append(bleu_score)\n",
        "\n",
        "        total_steps += config.summary_steps\n",
        "        \n",
        "        print('r', 'val[{}] loss: {:.3f}, Perplexity: {:.3f}, blue score: {:.3f}       '.format(total_steps, val_loss, np.exp(val_loss),bleu_score))\n",
        "\n",
        "        # save the model and result\n",
        "        if total_steps % config.store_steps == 0 or total_steps >= config.num_steps:\n",
        "            save_model(model, config.store_model_path, total_steps)\n",
        "        \n",
        "    return train_losses, val_losses, bleu_scores, result"
      ]
    },
    {
      "source": [
        "# Configuration"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Configurations:\n",
        "    def __init__(self):\n",
        "        self.batch_size = 60\n",
        "        self.emb_dim = 256\n",
        "        self.dec_hid_dim = 512\n",
        "        self.enc_hid_dim = 512\n",
        "        self.n_layers = 3\n",
        "        self.dropout = 0.5\n",
        "        self.learning_rate = 0.0005\n",
        "        self.max_output_len = 50\n",
        "        self.num_steps = 12000                          # total training times\n",
        "        self.store_steps = 300                          # save the model after every 'store_steps'\n",
        "        self.summary_steps = 300                        # test for every 'summary_steps' to see if it's over-fitting\n",
        "        self.load_model = False                         # determine if load the model\n",
        "        self.store_model_path = './ckpt'                # the path for storing the model\n",
        "        self.load_model_path = None                     # the path for loading the model e.g. \"./ckpt/model_{step}\" \n",
        "        self.data_path = './cmn-eng'                    # the path for getting the data\n",
        "        self.attention = True                           # use Attention or not\n",
        "        self.teacher_forcing_ratio = 0.5"
      ]
    },
    {
      "source": [
        "# Start Training"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    config = Configurations()\n",
        "    print('config:\\n', vars(config))\n",
        "    train_losses, val_losses, bleu_scores,result = train_process(config)"
      ]
    },
    {
      "source": [
        "# Start Testing"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Before testing, go to config to set up the path for loading the model\n",
        "if __name__ == '__main__':\n",
        "  config = configurations()\n",
        "  print ('config:\\n', vars(config))\n",
        "  test_loss, bleu_score = test_process(config)\n",
        "  print (f'test loss: {test_loss}, bleu_score: {bleu_score}')"
      ]
    }
  ]
}